{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '/home/hduser/Downloads/mysql-connector-j-8.1.0.jar pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']='--driver-class-path /usr/local/sqoop/lib/mysql-connector-java-5.1.47-bin.jar \\\n",
    "--jars /usr/local/sqoop/lib/mysql-connector-java-5.1.47/mysql-connector-java-5.1.47-bin.jar \\\n",
    "pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName('Read and write MySQL tables')\\\n",
    "    .master('local[*]')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start all deemans and then start mysql in terminal and make sure you have created the table before executing this\n",
    "#Loading the Dataframe from a table in local mysql database hari_db\n",
    "#'hari_db' after 3306/ is the database name,this database contains the required tables\n",
    "empDF = spark.read.format('jdbc')\\\n",
    "    .option('url', 'jdbc:mysql://localhost:3306/hari_db?useSSL=False')\\\n",
    "    .option('user', 'root')\\\n",
    "    .option('password', 'root')\\\n",
    "    .option('dbtable', 'emp')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.createOrReplaceTempView('emp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res_df = spark.sql(\"\"\"select a.empno as emp_no,a.ename as emp_ename,b.empno as mgr_empno,b.ename as mgr_ename,\n",
    "                      c.emp_cnt as mgr_tm_cnt\n",
    "                      from emp a,emp b,\n",
    "                      (select a.mgr,count(a.empno) as emp_cnt\n",
    "                       from emp a,emp b where a.mgr=b.empno group by a.mgr) c\n",
    "                       where a.mgr = b.empno and b.empno = c.mgr \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the New Table emp_mgr_cnt and storing it in the local database named hari_db\n",
    "res_df.write.format('jdbc')\\\n",
    ".mode('overwrite')\\\n",
    ".option('url','jdbc:mysql://localhost:3306/hari_db?useSSL=False')\\\n",
    ".option('user', 'root')\\\n",
    ".option('password', 'root')\\\n",
    ".option('dbtable', 'emp_mgr_cnt')\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
